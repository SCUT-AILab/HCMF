We thank all the reviewers for the comments and suggestions. We take the comments seriously and will carefully revise the paper.


To Reviewer 1:

Q: Comparisons with more fusion methods.
A: As suggested, we compared with the Bayesian-based method [1] suggested by you. Note that [1] requires the same dimensionality of features for fusing, which, however, is not directly applicable in many real world applications. Based on ResNet, recall that the global-pooling layer generates 64-dimension features. We thus choose the global-pooling layer's feature of all residual-based networks used in Table 4 as the input of the joint Bayesian method [1]. In this way, we have 50000 instances with feature dimension = 64 for total 10 classes. The accuracy of Joint Bayesian [1] is 93.8; While the accuracy of our method is 95.0, which outperforms Joint Bayesian. More importantly, because our method does not require the input feature to be of the same dimension, our method can be applied to a broader range of applications.

- Other two minor issues:
(i) In terms of efficiency, our method and RCEC are the two most efficient methods (See Table 2). The two algorithms have the similar range of running time. Nevertheless, our method outperforms RCEC in terms of accuracy for all the five datasets (See Table 1).

(ii) Our method and ResNeXt are two different kinds of methods for image classification. With the proposed fusion scheme, our method takes ResNeXt as additional input, and it outperforms ResNeXt by about 3%. Thus, the proposed method is significant and worthwhile. 

We will cite [1] and discuss more methods in our revised version.

[1] Bayesian face revisited: A joint formulation. In ECCV, 2012

To Reviewer 2:

Thanks for your comments that the paper proposes a new way for information fusion. We also appreciate your suggestions for improving the paper.

Your major concern is that we should add three more experiments. 
To address the concerns, we conducted all the suggested experiments and report the results on CIFAR-10 in this rebuttal due to space limit.

Q1. SVM-prediction V.S. CNN-prediction as the input of late fusion.

A. We used the SVM prediction as input in our original submission because some compared algorithms cannot take CNN predictions as input (see L801 to L803 for our justifications). Nevertheless, we additionally used CNN-predictions learned by end-to-end CNN models as the input of late fusion. By fusing the CNN-predictions, our method achieves an accuracy of 95.93, which is much better than the fusion of SVM-predictions (95.11). RCEC and average fusion are two other methods that can use CNN predictions as input, and their accuracies are 95.36 and 94.97, respectively. Again, our method outperforms the compared methods.

Q2. Neural network based fusion V.S. Our method.

A. Directly training a fusion model that fuses several CNN models may be intractable due to excessive GPU memory consumption. We thus perform the following two alternative experiments to respond your comment.

---(1) Feature fusion via CNN
We conducted an experiment in which we performed the fusion at the last pooling layer of all CNN models. Two strategies were used in our experiment. In the first setting, the features are flattened and concatenated (denoted as 'concatenation'). In the second setting, we apply max-pooling across channels (denoted by 'max pooling').
The results (in accuracy) are as follows:
| concatenation | max pooling | ours |
| --- 95.21 --- | -- 94.93 -- | 95.93|

---(2) Late fusion via CNN
We conducted the experiment in which we perform the fusion of different CNNs is performed at the fully connected layer. Below are the experiment results.
| CNN weighting | average | ours |
| --- 95.26 --- |- 95.12 -| 95.93| 

Based on the above comparisons, our method still achieves the best performance among all baselines.

Q3. The impact of the number of instances.

A. We performed an experiment by setting the different number of instances (number of rows in matrix L). In this experiment, SVM-predictions are used as fusion input. The experiment results (in accuracy) are listed below:
|#instances| 1000  | 2000 | 4000 | 6000 | 8000 | 10000 |
| Accuracy | 94.94 |95.10 |95.12 |95.10 |95.11 | 95.11 |

From the above comparison, our method achieves stable performance when the number is greater than 2000. 

We will include the above experimental results into the revised paper.


For Reviewer 3:

Thanks for your kind advice. We will revise the caption.
The C represents the number of classes. There are usually not many classes in most real-world applications. In this situation, the complexity is acceptable. If C is quite large for some specific problems (e.g. YouTube-8M includes 4716 classes). A trade-off method may be splitting all classes into multiple parts, and then performing late fusion on each part and finally merging together. The above discussions will be included in our revision.